---
layout: post
title: Large Language Models - LLMs
mathjax: true
tags:
- NLP
- LLM
- Transformer
categories: DeepLearning
description: 
---

## Large Language Models là gì?

Large Language Models (LLMs) là các mô hình ngôn ngữ dựa trên kiến trúc Transfomer có lượng tham số cực lớn và huấn luyện trên tập dữ liệu cực lớn để hiểu tạo tạo văn bản. Các ứng dụng LLM hiện nay chủ yếu cho việc sinh văn bản như chat bot, trợ lý ảo, viết lách, sinh mã (code). Những mô hình LLMs phổ biến hiện nay (2024) gồm GPT của OpenAI, LaMDA của Google, Gemimi (phát triển từ LaMDA) của Google, LLaMA của Facebook. 

## GPT

[Paper GPT - Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

GPT là mô hình ngôn ngữ được Open AI công bố năm 2018. Khác với BERT, GPT sử dụng autogregressive training để dự đoán từ tiếp theo trong một chuỗi. Thế mạnh của GPT là khả năng tạo sinh ra một câu văn mạch lạc dựa trên ngữ cảnh trước đó.

### Kiến trúc và huấn luyện GPT
- Transformer-based Architecture: Tương tự BERT, GPT cũng sử dụng Transformer, tuy nhiên nó tập trung vào phần decoder với mục đích là dự đoán từ tiếp theo theo trình tự từ trái sang phải.
- Unidirectional Context: Đối lập với Bidirectional training của BERT, GPT dự đoán từ tiếp theo trong chuỗi, việc tập trung này phù hợp để giải quyết các task như text generation, summarization.
- Pre-training and Fine-tuning: Tương tự như BERT, GPT cũng thực hiện 2 quá trình huấn luyện gồm Pre-training với một lượng dữ liệu khổng lồ, và sau đó tiếp tục train với Fine-tuning để tinh chỉnh cho các task phù hợp.


### Các cải tiến và khác biệt so với BERT

- Autoregressive vs. Bidirectional: Bằng việt sử dụng Autoregressive, GPT được huấn luyện để dự đoán từ tiếp theo với ngữ nghĩa cho trước, việc này là phù hợp với bài toán sinh văn bản trong chat hoặc summary (dựa trên hội thoại hoặc văn bản cho trước, sinh từ mới dựa vào các từ đã sinh). Đối với Bidirectional, BERT có thể đọc theo 2 chiều 1 câu/đoạn văn để hiểu thấu đáo văn bảo,giúp hỏi đáp hoặc các tác vụ yêu cầu hiểu văn bản.
- Zero-shot Learning: Không giống như BERT, GPT có khả năng tổng quát hóa mạnh mẽ trên các tác vụ mà không cần tinh chỉnh sâu rộng, điều này dẫn đến sự phát triển của GPT-2 và GPT-3, mở rộng đáng kể khái niệm này.
<!-- 
### Các tiến bộ sau GPT

- GPT-2 và GPT-3: Mention the scaled-up versions, including their ability to handle a broader range of tasks with minimal fine-tuning, contributing to the rise of AI assistants and large-scale natural language understanding models.
- GPT in Industry: Briefly touch on how GPT models are integrated into real-world applications like chatbots, text summarization, and even creative tasks like poetry and music composition. -->

### So sánh BERT và GPT
#### Áp dụng
- Nên chọn mô hình nào?: Không có mô hình nào tốt hơn, chỉ có mô hình phù hợp hơn, BERT sẽ phù hợp cho các tác vụ phân tích, hiểu văn bản như phân loại, hỏi đáp, NER,... Trong khi GPT phù hợp cho các tác vụ sinh văn bản như chat hoặc tóm tắt.

#### Linh hoạt
- BERT: Mạnh mẽ cho các task cụ thể cần fine-tuning, yêu cầu pre-training và tuning cho các task cụ thể.
- GPT: Rất linh hoạt cho các tác vụ few-shot hoặc zero-shot learning, nhưng khó thể tinh chỉnh cho các tác vụ cụ thể.

#### Mở rộng:
- BERT: Có khả năng mở rộng tốt nhưng tốn nhiều tài nguyên hơn do tính hai chiều, khiến quá trình suy luận chậm hơn.
- GPT: Có khả năng mở rộng tốt với khả năng hồi quy tự động nhưng lại ngốn nhiều tài nguyên trong các mô hình lớn hơn.


## LaMDA: Language Models for Dialog Applications
[Paper LaMDA](https://arxiv.org/pdf/2201.08239)

LaMDA được xây dựng dựa trên kiến trúc Transformer tương tự GPT, tuy nhiên được train trên tập dữ liệu về Dialoge, tập trung cho tác vụ sinh văn bản trong hội thoại với khả năng ghi nhớ thông tin cho các đoạn hội thoại dài với độ chính xác cao. LaMDA nhấn mạnh vào khả năng phản hồi chất lượng, an toàn và có cơ sở (Quality, Safety and Groundedness).

## LLaMA

[Paper LLaMA](https://arxiv.org/pdf/2302.13971)

LLaMA có thể là mô hình LLM đặt biệt nhất khi so sánh với GPT hay LaMDA, đơn giản vì nó... miễn phí (opensource). LLaMA có kích thước mô hình nhỏ hơn (từ 7B, 65B parameters), nhưng vấn rất mạnh mẽ và dễ dàng fine-tuning. Người dùng có thể dễ dạng triển khai trên máy cá nhân thậm chí laptop. Phù hợp cho việc học tập và nghiên cứu về LLM và các ứng dụng LLM.

## các mô hình khác và ứng dụng 

## Demo việc dùng 1 LLM

## References 

