---
layout: post
title: Perceptron, Neural Network, Deep Neural Network
mathjax: true
tags:
- Classification
categories: BasicMachineLearning
description: Neural Network, phần quan trọng nhất của Deep learning đã ra đời như thế nào?
---

# Perceptron 

> Bí thuật của trí tuệ nhân tạo hiện đại, nền tảng của nền tảng!

**Perceptron** là một trong những thuật toán sơ khai nhất để thực hiện phép phân loại 2 lớp, ý tưởng ban đầu là vẽ ra một đường phân loại (đường thẳng trên mặt phẳng, mặt phẳng trong không gian hay siêu phẳng (*hyperplane*) trong không gian đa chiều).

![Ví dụ về phân loại 2 lớp trong mặt phẳng](/MLDL/assets/img/perceptron_1.png)

Trong hình minh hoạ trên, ta thấy phân loại 2 lớp của các điểm đỏ và xanh rất rõ ràng, có thể có nhiều đường thẳng giúp phân loại 2 lớp này, bài toán đơn giản này tưởng chừng quá dễ để giải và không phù hợp với thực tế vì nhiều bài toán, các điểm phân loại không tách biệt mà xen lẫn vào nhau. Tuy nhiên, nó là nền tảng chính để phát triển các thuật toán mạng neural sau nay, tương tự với máy tính hiện đại chỉ hoạt động bằng các phép toán luận lý cơ bản trên 2 bit giá trị `1` và `0` rõ ràng nhưng có thể giải quyết các bài toán phức tạp.

## Thuật toán Peceptron (PLA)

Thuật toán _Perceptron Learning Algorithm_ (PLA) có ý tưởng đơn giản là tìm một đường thẳng để phân chia mặt phẳng 2 chiều thành 2 phần, phân loại các điểm dữ liệu vào 2 phần này. Đối với không gian nhiều chiều hơn, thuật toán sẽ tìm 1 siêu phẳng (_hyperplane_) để phân loại trong không gian.

Công thức của một đường thẳng trong mặt phẳng \\(Oxy\\) sẽ là 

\\[ax + by + c = 0\\]


Tuy nhiên, để phân biệt giữa nhãn và tham số nhiều chiều, ta sẽ gọi 2 trục x và y trong mặt phẳng là \\(x_1\\) và \\(x_2\\). 2 tham số lúc này sẽ được viết thành \\(w_1\\) và \\(w_2\\), công thức của 1 đường thẳng trong không gian 2 chiều mới \\(Ox_1x_2\\) được viết lại là:

\\[f(x) = w_1x_1 + w_1x_2 + c = 0\\]

Ta được biết rằng, đường thẳng \\(f(x)\\) này phân cách mặt phẳng thành 2 nữa mặt phẳng, các điểm dữ liệu ở cùng nữa mặt phẳng khi thay vào giá trị của đường thẳng sẽ cùng dấu (âm hoặc dương). 
Thuật toán PLA sẽ tìm cách tìm các giá trị của tham số \\(w\\) và \\(b\\) sao cho tất các các điểm trên cùng lớp sẽ có cùng dấu. 

Để đơn giản hơn, người ta gán cho 2 lớp với giá trị âm và dương, nếu điểm nào trong lớp có giá trị khác với giá trị mặc định của lớp đó thì cần sửa lại đường thẳng. Mã giả của thuật toán như sau:

```
1. Khởi tạo 1 đường thẳng bất kỳ bằng cách gán giá trị w và b ngẫu nhiên

2. Lặp các điểm cho đến khi tìm được kết quả:
    2.1 Kiểm tra xem các điểm trong lớp có bị phân loại sai không
    2.2 Nếu có, cập nhật w và b theo công thức: 
        w = w + y * x
        b = b + y
3. Khi không còn điểm nào sai, xuất ra kết quả w và b

```

Tại sao lại là `w = w + y * x` và `b = b + y`?

Hiểu đơn giản, đây là cách _"sai ở đâu, sửa ở đó"_.

**Giải thích**: việc chỉ update các điểm dữ liệu phân loại sai, nghĩa là y và và giá trị của \\(y\\) sẽ trái dấu với \\(w*x+b\\), việc cộng tham số với giá trị trái dấu của nó sẽ làm nó nhỏ dần và tiến về 0 hoặc ngược dấu với giá trị hiện tại. Khi đó, tất cả các điểm dữ liệu sai sẽ được lặp lại và sửa cho đến khi đúng.

Thuật toán với Python
```python 
import numpy as np

def perceptron(X, y, lr=0.1, epochs=1000):
    """
    Thuật toán Perceptron Learning Algorithm (PLA)

    Args:
        X: Ma trận dữ liệu đầu vào (mỗi hàng là một điểm dữ liệu)
        y: Vector nhãn lớp (-1 hoặc 1)
        lr: Learning rate (tốc độ học)
        epochs: Số lượng epochs (vòng lặp huấn luyện)

    Returns:
        w: Vector trọng số đã học được
        b: Bias đã học được
    """
    m, n = X.shape  # Lấy số lượng điểm dữ liệu và số lượng đặc trưng
    w = np.zeros(n)  # Khởi tạo vector trọng số
    b = 0  # Khởi tạo bias

    for _ in range(epochs):
        for i in range(m):
            x = X[i]
            y_pred = np.sign(np.dot(w, x) + b)  # Tính toán dự đoán

            if y_pred != y[i]:  # Nếu dự đoán sai
                w += lr * y[i] * x  # Cập nhật trọng số
                b += lr * y[i]  # Cập nhật bias

    return w, b

# Ví dụ sử dụng:
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y = np.array([-1, -1, 1, 1])
w, b = perceptron(X, y)

print("Trọng số đã học được:", w)
print("Bias đã học được:", b)
```
Ở thuật toán python này, tôi có thêm biến learning rate `lr` để giúp việc update tham số từ từ chậm hơn, tránh việc số nhảy loạn khi x lớn, khó hội tụ.


# Neural Network 
## Multilayer perceptron 
> Từ đường thẳng đến đường cong

Perceptron đã giải quyết được bài toán phân loại tuyến tính (đường thẳng) rồi, nhưng chúng ta mong muốn có một mô hình mạnh mẽ hơn, có thể giải quyết được các bài toán phân loại mà các đường phân loại (hay hyperplane trong không gian đa chiều) là những đường cong phức tạp không theo phương trình nhất định hoăc là các điểm dữ liệu có thể không tách biệt hoàn toàn. Cách nào để giải quyết được nó đây?

![Ví dụ về phân loại 2 lớp trong mặt phẳng với đường phi tuyến](/MLDL/assets/img/perceptron_2.png)


Ý tưởng đơn giản cho việc ước lượng 1 đường cho để tách biệt 2 phần dữ liệu là vẽ nhiều đoạn thẳng, đoạn cong và các tia, các đoạn càng nhiều thì đường cong phân tách sẽ càng mượt.
Vậy cách nào để biến đường thẳng tuyến tính thành các đoạn thẳng hay đoạn cong? câu trả lời là dùng thêm các hàm phi tuyến tính làm hàm kích hoạt (activation function) ở đầu ra của các perceptron, tiêu biểu như hàm Relu, có công thức là 
\\[y = x (nếu x > 0) \\\ 
y = 0 (nếu x <= 0)\\]

![Hàm Relu](/MLDL/assets/img/perceptron_3.png)

![Hàm Sigmoid](/MLDL/assets/img/LRSigmoid.gif)


Ngoài Relu thì ta có thể sử dụng nhiều hàm phi tuyến khác như Sin, Sigmoid (có đề cập ở bài Logistic Regression, Tanh,...) để giúp bẻ cong hoặc giới hạn sự tuyến tính của output của 1 perceptron, kết hợp nhiều perceptron có hàm kích hoạt sẽ giúp mô hình có thể có ước lượng bất kỳ đường cong nào.

![Mô hình multilayer Perceptron](/MLDL/assets/img/perceptron_4.png)

Ngoài việc tăng số lượng perceptron, ta có thể tăng nhiều lớp perceptron với output của lớp trước là input của lớp sau, các mô hình sẽ học được nhiều thông tin và mối quan hệ phức tạp hơn giữa các input và các perceptron.

Một mạng lưới các perceptron liên kết với nhau được gọi là mạng nơ-ron hay neural network.

## Loss function
> "Thước Đo" Sai Số của Neural network

Trong thuật toán PLA, các tham số được điều chỉnh để đường phân loại (hoặc hyperplane trong không gian đa chiều) sang hướng của điểm dữ liệu bị phân loại sai, ta cũng không biết nó sai nhiều hay ít (ở xa hay gần). Đồng thời, với các tập dữ liệu có sự trùng lặp (overlap) giữa các class như hình minh hoạ dưới, rất khó để tìm ra 1 đường phân loại chính xác 2 lớp.

![Dữ liệu phân loại có overlap](/MLDL/assets/img/perceptron_5.png)

Hàm mất mát (loss function) cho phép mô hình biết được các điểm dữ liệu nào là sai ít hoặc sai nhiều, các điểm sai nào là chấp nhận được để nhanh chóng tìm ra đường phân loại tối ưu.

### Các loại Loss Function phổ biến:

Tùy thuộc vào loại bài toán (phân loại hay hồi quy) và đặc thù của dữ liệu, chúng ta có thể lựa chọn các loại Loss Function khác nhau. Dưới đây là một số hàm mất mát phổ biến:

Mean Squared Error (MSE): Thường được sử dụng trong các bài toán hồi quy, MSE tính toán trung bình bình phương của sai số giữa giá trị dự đoán và giá trị thực tế.

\\[\sum_{i=1}^{D}(x_i-y_i)^2]\\

Mean Absolute Error (MAE): Tương tự như MSE, nhưng MAE tính trung bình giá trị tuyệt đối của sai số. MAE ít bị ảnh hưởng bởi các điểm dữ liệu ngoại lai (outliers) hơn so với MSE.

\\[\sum_{i=1}^{D}|x_i-y_i|]\\

Cross-Entropy Loss: Thường được sử dụng trong các bài toán phân loại, Cross-Entropy Loss đo lường mức độ khác biệt giữa phân phối xác suất dự đoán của mô hình và phân phối xác suất thực tế của các lớp.

\\[-\sum_{c=1}^My_{o,c}\log(p_{o,c})\\]

Hinge Loss: Được sử dụng trong các mô hình Support Vector Machine (SVM), Hinge Loss khuyến khích mô hình tìm ra một lề (margin) lớn nhất có thể giữa các lớp.

Việc lựa chọn Loss Function phù hợp là một yếu tố quan trọng ảnh hưởng đến hiệu suất của mô hình. Không có một Loss Function nào là tốt nhất cho mọi bài toán, việc lựa chọn phụ thuộc vào đặc thù của dữ liệu và mục tiêu của bài toán.


## Backpropagation
> Bí quyết "Luyện tập" của Neural network

**Backpropagation** là một thuật toán tối ưu hóa dựa trên gradient (gradient-based optimization) được sử dụng để điều chỉnh các trọng số \\(w\\) và bias \\(b\\) của MLP trong quá trình huấn luyện. Nó hoạt động bằng cách tính toán gradient của hàm mất mát (loss function) theo từng trọng số và bias, sau đó cập nhật các tham số này theo hướng ngược với gradient để giảm thiểu sai số giữa dự đoán của mô hình và giá trị thực tế.

Quá trình Backpropagation:

Feedforward (Lan truyền xuôi): Đầu tiên, dữ liệu đầu vào được đưa vào lớp đầu vào của MLP và lan truyền qua các lớp ẩn đến lớp đầu ra, tạo ra một dự đoán.

Tính toán sai số (Error Calculation): Sai số giữa dự đoán của mô hình và giá trị thực tế được tính toán bằng hàm mất mát. Hàm mất mát thường được sử dụng là Mean Squared Error (MSE) cho bài toán hồi quy hoặc Cross-Entropy Loss cho bài toán phân loại.

Backpropagation (Lan truyền ngược): Gradient của hàm mất mát theo từng trọng số và bias được tính toán bằng cách sử dụng quy tắc chuỗi (chain rule) của đạo hàm. Gradient này cho biết hướng và độ lớn cần thiết để cập nhật các tham số nhằm giảm thiểu sai số.

Cập nhật trọng số (Weight Update): Trọng số và bias được cập nhật theo hướng ngược với gradient, với một hệ số tỷ lệ được gọi là learning rate (tốc độ học). Learning rate quyết định mức độ "nhanh" mà mô hình điều chỉnh các tham số.

Ý nghĩa của Backpropagation:

Backpropagation đóng vai trò như một "huấn luyện viên" cho MLP, giúp nó "học" từ những sai lầm của mình. Bằng cách liên tục cập nhật trọng số và bias dựa trên thông tin phản hồi từ sai số, MLP dần dần cải thiện khả năng dự đoán của mình.

Tầm quan trọng của Backpropagation:

Backpropagation là một trong những đột phá quan trọng nhất trong lịch sử phát triển của mạng nơ-ron. Nó đã mở ra cánh cửa cho việc huấn luyện các mạng nơ-ron sâu (deep neural networks) với nhiều lớp ẩn, giúp chúng đạt được hiệu suất cao trong nhiều tác vụ khác nhau.


# Deep Neural Network 

# Tài liệu tham khảo  
1. [Stanford CS229 Lecture Notes (Notes 3)](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
2. [Convex Optimization](http://stanford.edu/~boyd/cvxbook/) – Boyd and Vandenberghe, Cambridge University Press, 2004.
3. [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576)
4. [Machine Learning Cơ bản](http://machinelearningcoban.com)
