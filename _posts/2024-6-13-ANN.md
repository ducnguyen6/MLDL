---
layout: post
title: Perceptron, Neural Network, Deep Neural Network
mathjax: true
tags:
- Classification
categories: BasicMachineLearning
description: Neural Network, phần quan trọng nhất của Deep learning đã ra đời như thế nào?
---

# Perceptron 

> Bí thuật của trí tuệ nhân tạo hiện đại, nền tảng của nền tảng!

**Perceptron** là một trong những thuật toán sơ khai nhất để thực hiện phép phân loại 2 lớp, ý tưởng ban đầu là vẽ ra một đường phân loại (đường thẳng trên mặt phẳng, mặt phẳng trong không gian hay siêu phẳng (*hyperplane*) trong không gian đa chiều).

![Ví dụ về phân loại 2 lớp trong mặt phẳng](/MLDL/assets/img/perceptron_1.png)

Trong hình minh hoạ trên, ta thấy phân loại 2 lớp của các điểm đỏ và xanh rất rõ ràng, có thể có nhiều đường thẳng giúp phân loại 2 lớp này, bài toán đơn giản này tưởng chừng quá dễ để giải và không phù hợp với thực tế vì nhiều bài toán, các điểm phân loại không tách biệt mà xen lẫn vào nhau. Tuy nhiên, nó là nền tảng chính để phát triển các thuật toán mạng neural sau nay, tương tự với máy tính hiện đại chỉ hoạt động bằng các phép toán luận lý cơ bản trên 2 bit giá trị `1` và `0` rõ ràng nhưng có thể giải quyết các bài toán phức tạp.

## Thuật toán Peceptron (PLA)

Thuật toán _Perceptron Learning Algorithm_ (PLA) có ý tưởng đơn giản là tìm một đường thẳng để phân chia mặt phẳng 2 chiều thành 2 phần, phân loại các điểm dữ liệu vào 2 phần này. Đối với không gian nhiều chiều hơn, thuật toán sẽ tìm 1 siêu phẳng (_hyperplane_) để phân loại trong không gian.

Công thức của một đường thẳng trong mặt phẳng \\(Oxy\\) sẽ là 

\\[ax + by + c = 0\\]


Tuy nhiên, để phân biệt giữa nhãn và tham số nhiều chiều, ta sẽ gọi 2 trục x và y trong mặt phẳng là \\(x_1\\) và \\(x_2\\). 2 tham số lúc này sẽ được viết thành \\(w_1\\) và \\(w_2\\), công thức của 1 đường thẳng trong không gian 2 chiều mới \\(Ox_1x_2\\) được viết lại là:

\\[f(x) = w_1x_1 + w_1x_2 + c = 0\\]

Ta được biết rằng, đường thẳng \\(f(x)\\) này phân cách mặt phẳng thành 2 nữa mặt phẳng, các điểm dữ liệu ở cùng nữa mặt phẳng khi thay vào giá trị của đường thẳng sẽ cùng dấu (âm hoặc dương). 
Thuật toán PLA sẽ tìm cách tìm các giá trị của tham số \\(w\\) và \\(b\\) sao cho tất các các điểm trên cùng lớp sẽ có cùng dấu. 

Để đơn giản hơn, người ta gán cho 2 lớp với giá trị âm và dương, nếu điểm nào trong lớp có giá trị khác với giá trị mặc định của lớp đó thì cần sửa lại đường thẳng. Mã giả của thuật toán như sau:

```
1. Khởi tạo 1 đường thẳng bất kỳ bằng cách gán giá trị w và b ngẫu nhiên

2. Lặp các điểm cho đến khi tìm được kết quả:
    2.1 Kiểm tra xem các điểm trong lớp có bị phân loại sai không
    2.2 Nếu có, cập nhật w và b theo công thức: 
        w = w + y * x
        b = b + y
3. Khi không còn điểm nào sai, xuất ra kết quả w và b

```

Tại sao lại là `w = w + y * x` và `b = b + y`?

Hiểu đơn giản, đây là cách _"sai ở đâu, sửa ở đó"_.

Ví dụ đường thẳng có tham số $w_1$=`3` $w_2$=`2`, $b$=`1`. 

Cho 2 điểm giữ liệu $x_1 = 1$, $x_2=-1$, và nhãn 
$y=-1$, thay vào đường thẳng ta có được.

$$ f(x) = 3x_1 + 2x_2 + 1$$
$$= 3*1 + 2*-1 + 1 $$
$$= 2$$

Giá trị $2$ khác dấu với $y=-1$ đã cho, ta thực hiện cập nhật lại dữ liệu:

$w_1 = w_1 + y*x_1 = 3 + -1 * 1 = 2$

$w_2 = w_2 + y*x_2 = 2 + -1 * -1 = 3$

$b=b+y=1+-1=0$

đường thẳng mới sẽ là:
$$ f(x) = 2x_1 + 3x_2 + 0$$

với đườn thẳng này, ta thay giá trị điểm trên, ta sẽ được $$ f(x) = 2x_1 + 3x_2 + 0$$
$$= 2*1 + 3*-1 0 $$
$$=-1$$
Bằng đúng giá trị của $y$

Tại sao vậy?



# Neural Network 

# Neural Network 

# Tài liệu tham khảo  
1. [Stanford CS229 Lecture Notes (Notes 3)](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
2. [Convex Optimization](http://stanford.edu/~boyd/cvxbook/) – Boyd and Vandenberghe, Cambridge University Press, 2004.
3. [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576)
4. [Machine Learning Cơ bản](http://machinelearningcoban.com)
