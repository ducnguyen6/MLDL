---
layout: post
title: Perceptron, Neural Network, Deep Neural Network
mathjax: true
tags:
- Classification
categories: BasicMachineLearning
description: Neural Network, phần quan trọng nhất của Deep learning đã ra đời như thế nào?
---

# Perceptron 

> Bí thuật của trí tuệ nhân tạo hiện đại, nền tảng của nền tảng!

**Perceptron** là một trong những thuật toán sơ khai nhất để thực hiện phép phân loại 2 lớp, ý tưởng ban đầu là vẽ ra một đường phân loại (đường thẳng trên mặt phẳng, mặt phẳng trong không gian hay siêu phẳng (*hyperplane*) trong không gian đa chiều).

![Ví dụ về phân loại 2 lớp trong mặt phẳng](/MLDL/assets/img/perceptron_1.png)

Trong hình minh hoạ trên, ta thấy phân loại 2 lớp của các điểm đỏ và xanh rất rõ ràng, có thể có nhiều đường thẳng giúp phân loại 2 lớp này, bài toán đơn giản này tưởng chừng quá dễ để giải và không phù hợp với thực tế vì nhiều bài toán, các điểm phân loại không tách biệt mà xen lẫn vào nhau. Tuy nhiên, nó là nền tảng chính để phát triển các thuật toán mạng neural sau nay, tương tự với máy tính hiện đại chỉ hoạt động bằng các phép toán luận lý cơ bản trên 2 bit giá trị `1` và `0` rõ ràng nhưng có thể giải quyết các bài toán phức tạp.

## Thuật toán Peceptron (PLA)

Thuật toán _Perceptron Learning Algorithm_ (PLA) có ý tưởng đơn giản là tìm một đường thẳng để phân chia mặt phẳng 2 chiều thành 2 phần, phân loại các điểm dữ liệu vào 2 phần này. Đối với không gian nhiều chiều hơn, thuật toán sẽ tìm 1 siêu phẳng (_hyperplane_) để phân loại trong không gian.

Công thức của một đường thẳng trong mặt phẳng \\(Oxy\\) sẽ là 

\\[ax + by + c = 0\\]


Tuy nhiên, để phân biệt giữa nhãn và tham số nhiều chiều, ta sẽ gọi 2 trục x và y trong mặt phẳng là \\(x_1\\) và \\(x_2\\). 2 tham số lúc này sẽ được viết thành \\(w_1\\) và \\(w_2\\), công thức của 1 đường thẳng trong không gian 2 chiều mới \\(Ox_1x_2\\) được viết lại là:

\\[f(x) = w_1x_1 + w_1x_2 + c = 0\\]

Ta được biết rằng, đường thẳng \\(f(x)\\) này phân cách mặt phẳng thành 2 nữa mặt phẳng, các điểm dữ liệu ở cùng nữa mặt phẳng khi thay vào giá trị của đường thẳng sẽ cùng dấu (âm hoặc dương). 
Thuật toán PLA sẽ tìm cách tìm các giá trị của tham số \\(w\\) và \\(b\\) sao cho tất các các điểm trên cùng lớp sẽ có cùng dấu. 

Để đơn giản hơn, người ta gán cho 2 lớp với giá trị âm và dương, nếu điểm nào trong lớp có giá trị khác với giá trị mặc định của lớp đó thì cần sửa lại đường thẳng. Mã giả của thuật toán như sau:

```
1. Khởi tạo 1 đường thẳng bất kỳ bằng cách gán giá trị w và b ngẫu nhiên

2. Lặp các điểm cho đến khi tìm được kết quả:
    2.1 Kiểm tra xem các điểm trong lớp có bị phân loại sai không
    2.2 Nếu có, cập nhật w và b theo công thức: 
        w = w + y * x
        b = b + y
3. Khi không còn điểm nào sai, xuất ra kết quả w và b

```

Tại sao lại là `w = w + y * x` và `b = b + y`?

Hiểu đơn giản, đây là cách _"sai ở đâu, sửa ở đó"_.

**Giải thích**: việc chỉ update các điểm dữ liệu phân loại sai, nghĩa là y và và giá trị của \\(y\\) sẽ trái dấu với \\(w*x+b\\), việc cộng tham số với giá trị trái dấu của nó sẽ làm nó nhỏ dần và tiến về 0 hoặc ngược dấu với giá trị hiện tại. Khi đó, tất cả các điểm dữ liệu sai sẽ được lặp lại và sửa cho đến khi đúng.

Thuật toán với Python
```python 
import numpy as np

def perceptron(X, y, lr=0.1, epochs=1000):
    """
    Thuật toán Perceptron Learning Algorithm (PLA)

    Args:
        X: Ma trận dữ liệu đầu vào (mỗi hàng là một điểm dữ liệu)
        y: Vector nhãn lớp (-1 hoặc 1)
        lr: Learning rate (tốc độ học)
        epochs: Số lượng epochs (vòng lặp huấn luyện)

    Returns:
        w: Vector trọng số đã học được
        b: Bias đã học được
    """
    m, n = X.shape  # Lấy số lượng điểm dữ liệu và số lượng đặc trưng
    w = np.zeros(n)  # Khởi tạo vector trọng số
    b = 0  # Khởi tạo bias

    for _ in range(epochs):
        for i in range(m):
            x = X[i]
            y_pred = np.sign(np.dot(w, x) + b)  # Tính toán dự đoán

            if y_pred != y[i]:  # Nếu dự đoán sai
                w += lr * y[i] * x  # Cập nhật trọng số
                b += lr * y[i]  # Cập nhật bias

    return w, b

# Ví dụ sử dụng:
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y = np.array([-1, -1, 1, 1])
w, b = perceptron(X, y)

print("Trọng số đã học được:", w)
print("Bias đã học được:", b)
```
Ở thuật toán python này, tôi có thêm biến learning rate `lr` để giúp việc update tham số từ từ chậm hơn, tránh việc số nhảy loạn khi x lớn, khó hội tụ.


# Neural Network 
## Multilayer perceptron 
> Từ đường thẳng đến đường cong

Perceptron đã giải quyết được bài toán phân loại tuyến tính (đường thẳng) rồi, nhưng chúng ta mong muốn có một mô hình mạnh mẽ hơn, có thể giải quyết được các bài toán phân loại mà các đường phân loại (hay hyperplane trong không gian đa chiều) là những đường cong phức tạp không theo phương trình nhất định hoăc là các điểm dữ liệu có thể không tách biệt hoàn toàn. Cách nào để giải quyết được nó đây?

![Ví dụ về phân loại 2 lớp trong mặt phẳng với đường phi tuyến](/MLDL/assets/img/perceptron_2.png)


Ý tưởng đơn giản cho việc ước lượng 1 đường cho để tách biệt 2 phần dữ liệu là vẽ nhiều đoạn thẳng, đoạn cong và các tia, các đoạn càng nhiều thì đường cong phân tách sẽ càng mượt.
Vậy cách nào để biến đường thẳng tuyến tính thành các đoạn thẳng hay đoạn cong? câu trả lời là dùng thêm các hàm phi tuyến tính làm hàm kích hoạt (activation function) ở đầu ra của các perceptron, tiêu biểu như hàm Relu, có công thức là 
\\[
y = x (nếu x > 0)
y = 0 (nếu x <= 0)

\\]

![Hàm Relu](/MLDL/assets/img/perceptron_3.png)

![Hàm Sigmoid](/MLDL/assets/img/LRSigmoid.gif)


Ngoài Relu thì ta có thể sử dụng nhiều hàm phi tuyến khác như Sin, Sigmoid (có đề cập ở bài Logistic Regression, Tanh,...) để giúp bẻ cong hoặc giới hạn sự tuyến tính của output của 1 perceptron, kết hợp nhiều perceptron có hàm kích hoạt sẽ giúp mô hình có thể có ước lượng bất kỳ đường cong nào.

![Mô hình multilayer Perceptron](/MLDL/assets/img/perceptron_4.png)

Ngoài việc tăng số lượng perceptron, ta có thể tăng nhiều lớp perceptron với output của lớp trước là input của lớp sau, các mô hình sẽ học được nhiều thông tin và mối quan hệ phức tạp hơn giữa các input và các perceptron.

## Backpropagation 


# Deep Neural Network 

# Tài liệu tham khảo  
1. [Stanford CS229 Lecture Notes (Notes 3)](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
2. [Convex Optimization](http://stanford.edu/~boyd/cvxbook/) – Boyd and Vandenberghe, Cambridge University Press, 2004.
3. [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576)
4. [Machine Learning Cơ bản](http://machinelearningcoban.com)
